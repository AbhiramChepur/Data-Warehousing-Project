{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 520 Data Curation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our goal for the project is to Extract data from a CSV file. \n",
    "## Transform the original schema of the file into the required schema with lesser number of columns and with column names changed. \n",
    "## Explore and Query the Data. \n",
    "## Save the data as JSON Documents in a Mongo-DB JSON Table. \n",
    "## All this is done using Spark in Python.\n",
    "\n",
    "\n",
    "\n",
    "## We have used the Medicare Open payments data from a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('CS 520').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading the csv File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"payments.csv\", header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the original schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Change_Type: string (nullable = true)\n",
      " |-- Covered_Recipient_Type: string (nullable = true)\n",
      " |-- Teaching_Hospital_CCN: string (nullable = true)\n",
      " |-- Teaching_Hospital_ID: string (nullable = true)\n",
      " |-- Teaching_Hospital_Name: string (nullable = true)\n",
      " |-- Physician_Profile_ID: string (nullable = true)\n",
      " |-- Physician_First_Name: string (nullable = true)\n",
      " |-- Physician_Middle_Name: string (nullable = true)\n",
      " |-- Physician_Last_Name: string (nullable = true)\n",
      " |-- Physician_Name_Suffix: string (nullable = true)\n",
      " |-- Recipient_Primary_Business_Street_Address_Line1: string (nullable = true)\n",
      " |-- Recipient_Primary_Business_Street_Address_Line2: string (nullable = true)\n",
      " |-- Recipient_City: string (nullable = true)\n",
      " |-- Recipient_State: string (nullable = true)\n",
      " |-- Recipient_Zip_Code: string (nullable = true)\n",
      " |-- Recipient_Country: string (nullable = true)\n",
      " |-- Recipient_Province: string (nullable = true)\n",
      " |-- Recipient_Postal_Code: string (nullable = true)\n",
      " |-- Physician_Primary_Type: string (nullable = true)\n",
      " |-- Physician_Specialty: string (nullable = true)\n",
      " |-- Physician_License_State_code1: string (nullable = true)\n",
      " |-- Physician_License_State_code2: string (nullable = true)\n",
      " |-- Physician_License_State_code3: string (nullable = true)\n",
      " |-- Physician_License_State_code4: string (nullable = true)\n",
      " |-- Physician_License_State_code5: string (nullable = true)\n",
      " |-- Submitting_Applicable_Manufacturer_or_Applicable_GPO_Name: string (nullable = true)\n",
      " |-- Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_ID: string (nullable = true)\n",
      " |-- Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Name: string (nullable = true)\n",
      " |-- Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_State: string (nullable = true)\n",
      " |-- Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Country: string (nullable = true)\n",
      " |-- Total_Amount_of_Payment_USDollars: string (nullable = true)\n",
      " |-- Date_of_Payment: string (nullable = true)\n",
      " |-- Number_of_Payments_Included_in_Total_Amount: string (nullable = true)\n",
      " |-- Form_of_Payment_or_Transfer_of_Value: string (nullable = true)\n",
      " |-- Nature_of_Payment_or_Transfer_of_Value: string (nullable = true)\n",
      " |-- City_of_Travel: string (nullable = true)\n",
      " |-- State_of_Travel: string (nullable = true)\n",
      " |-- Country_of_Travel: string (nullable = true)\n",
      " |-- Physician_Ownership_Indicator: string (nullable = true)\n",
      " |-- Third_Party_Payment_Recipient_Indicator: string (nullable = true)\n",
      " |-- Name_of_Third_Party_Entity_Receiving_Payment_or_Transfer_of_Value: string (nullable = true)\n",
      " |-- Charity_Indicator: string (nullable = true)\n",
      " |-- Third_Party_Equals_Covered_Recipient_Indicator: string (nullable = true)\n",
      " |-- Contextual_Information: string (nullable = true)\n",
      " |-- Delay_in_Publication_Indicator: string (nullable = true)\n",
      " |-- Record_ID: string (nullable = true)\n",
      " |-- Dispute_Status_for_Publication: string (nullable = true)\n",
      " |-- Related_Product_Indicator: string (nullable = true)\n",
      " |-- Covered_or_Noncovered_Indicator_1: string (nullable = true)\n",
      " |-- Indicate_Drug_or_Biological_or_Device_or_Medical_Supply_1: string (nullable = true)\n",
      " |-- Product_Category_or_Therapeutic_Area_1: string (nullable = true)\n",
      " |-- Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_1: string (nullable = true)\n",
      " |-- Associated_Drug_or_Biological_NDC_1: string (nullable = true)\n",
      " |-- Covered_or_Noncovered_Indicator_2: string (nullable = true)\n",
      " |-- Indicate_Drug_or_Biological_or_Device_or_Medical_Supply_2: string (nullable = true)\n",
      " |-- Product_Category_or_Therapeutic_Area_2: string (nullable = true)\n",
      " |-- Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_2: string (nullable = true)\n",
      " |-- Associated_Drug_or_Biological_NDC_2: string (nullable = true)\n",
      " |-- Covered_or_Noncovered_Indicator_3: string (nullable = true)\n",
      " |-- Indicate_Drug_or_Biological_or_Device_or_Medical_Supply_3: string (nullable = true)\n",
      " |-- Product_Category_or_Therapeutic_Area_3: string (nullable = true)\n",
      " |-- Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_3: string (nullable = true)\n",
      " |-- Associated_Drug_or_Biological_NDC_3: string (nullable = true)\n",
      " |-- Covered_or_Noncovered_Indicator_4: string (nullable = true)\n",
      " |-- Indicate_Drug_or_Biological_or_Device_or_Medical_Supply_4: string (nullable = true)\n",
      " |-- Product_Category_or_Therapeutic_Area_4: string (nullable = true)\n",
      " |-- Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_4: string (nullable = true)\n",
      " |-- Associated_Drug_or_Biological_NDC_4: string (nullable = true)\n",
      " |-- Covered_or_Noncovered_Indicator_5: string (nullable = true)\n",
      " |-- Indicate_Drug_or_Biological_or_Device_or_Medical_Supply_5: string (nullable = true)\n",
      " |-- Product_Category_or_Therapeutic_Area_5: string (nullable = true)\n",
      " |-- Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_5: string (nullable = true)\n",
      " |-- Associated_Drug_or_Biological_NDC_5: string (nullable = true)\n",
      " |-- Program_Year: string (nullable = true)\n",
      " |-- Payment_Publication_Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the data type of Amount from String to Double  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumn(\"amount\" , df[\"Total_Amount_of_Payment_USDollars\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Temporary Payments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.createGlobalTempView(\"payments1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also specify the schema while importing the file in the below manner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"physician_id\", StringType(), True),StructField(\"date_payment\", StringType(), True),StructField(\"record_id\", StringType(), True),StructField(\"payer\", StringType(), True),StructField(\"amount\", DoubleType(), True),StructField(\"physician_speciality\", StringType(), True),StructField(\"nature_of_payment\", StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting only the columns we want and also renaming the Columns as we want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = spark.sql(\"select Physician_Profile_ID as physician_id,Date_of_Payment as date_payment, Record_ID as record_id, Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Name as payer,amount, Physician_Specialty, Nature_of_Payment_or_Transfer_of_Value as Nature_of_payment from global_temp.payments1 where Physician_Profile_ID IS NOT NULL\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- physician_id: string (nullable = true)\n",
      " |-- date_payment: string (nullable = true)\n",
      " |-- record_id: string (nullable = true)\n",
      " |-- payer: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- Physician_Specialty: string (nullable = true)\n",
      " |-- Nature_of_payment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[physician_id: string, date_payment: string, record_id: string, payer: string, amount: double, Physician_Specialty: string, Nature_of_payment: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(physician_id='673985', date_payment='01/21/2016', record_id='346039414', payer='DFINE, Inc', amount=286.2, Physician_Specialty='Allopathic & Osteopathic Physicians|Anesthesiology', Nature_of_payment='Travel and Lodging')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing the temporary view with our new view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds.createOrReplaceGlobalTempView(\"payments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+----------+------+--------------------+------------------+\n",
      "|physician_id|date_payment|record_id|     payer|amount| Physician_Specialty| Nature_of_payment|\n",
      "+------------+------------+---------+----------+------+--------------------+------------------+\n",
      "|      673985|  01/21/2016|346039414|DFINE, Inc| 286.2|Allopathic & Oste...|Travel and Lodging|\n",
      "|      673985|  01/21/2016|346039416|DFINE, Inc|  25.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|      673985|  02/19/2016|346039418|DFINE, Inc| 27.27|Allopathic & Oste...|Travel and Lodging|\n",
      "|       93975|  04/15/2016|346039420|DFINE, Inc|  21.6|Allopathic & Oste...| Food and Beverage|\n",
      "|      275444|  04/29/2016|346039422|DFINE, Inc| 22.57|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  02/05/2016|346039424|DFINE, Inc| 780.7|Allopathic & Oste...|Travel and Lodging|\n",
      "|      132655|  02/05/2016|346039426|DFINE, Inc|  25.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|      132655|  02/19/2016|346039428|DFINE, Inc| 27.27|Allopathic & Oste...|Travel and Lodging|\n",
      "|      132655|  02/18/2016|346039430|DFINE, Inc| 114.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|      132655|  04/18/2016|346039432|DFINE, Inc|122.49|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  03/15/2016|346039434|DFINE, Inc| 103.5|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  09/06/2016|346039436|DFINE, Inc| 90.91|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  02/12/2016|346039438|DFINE, Inc| 90.87|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  02/13/2016|346039440|DFINE, Inc| 23.45|Allopathic & Oste...| Food and Beverage|\n",
      "|     1006832|  02/13/2016|346039442|DFINE, Inc|  32.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  01/26/2016|346039444|DFINE, Inc| 630.2|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  01/26/2016|346039446|DFINE, Inc|  25.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  02/12/2016|346039448|DFINE, Inc|  59.8|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  02/19/2016|346039450|DFINE, Inc| 27.27|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  02/12/2016|346039452|DFINE, Inc| 10.75|Allopathic & Oste...| Food and Beverage|\n",
      "+------------+------------+---------+----------+------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the type of date from string to date format and also changing format from mm/dd/yyyy to yyyy-mm-dd (unix timestamp) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "ds =ds.withColumn(\"date_payment\", to_date(unix_timestamp(ds[\"date_payment\"], \"MM/dd/yyyy\").cast(\"timestamp\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- physician_id: string (nullable = true)\n",
      " |-- date_payment: date (nullable = true)\n",
      " |-- record_id: string (nullable = true)\n",
      " |-- payer: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- Physician_Specialty: string (nullable = true)\n",
      " |-- Nature_of_payment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+----------+------+--------------------+------------------+\n",
      "|physician_id|date_payment|record_id|     payer|amount| Physician_Specialty| Nature_of_payment|\n",
      "+------------+------------+---------+----------+------+--------------------+------------------+\n",
      "|      673985|  2016-01-21|346039414|DFINE, Inc| 286.2|Allopathic & Oste...|Travel and Lodging|\n",
      "|      673985|  2016-01-21|346039416|DFINE, Inc|  25.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|      673985|  2016-02-19|346039418|DFINE, Inc| 27.27|Allopathic & Oste...|Travel and Lodging|\n",
      "|       93975|  2016-04-15|346039420|DFINE, Inc|  21.6|Allopathic & Oste...| Food and Beverage|\n",
      "|      275444|  2016-04-29|346039422|DFINE, Inc| 22.57|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  2016-02-05|346039424|DFINE, Inc| 780.7|Allopathic & Oste...|Travel and Lodging|\n",
      "|      132655|  2016-02-05|346039426|DFINE, Inc|  25.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|      132655|  2016-02-19|346039428|DFINE, Inc| 27.27|Allopathic & Oste...|Travel and Lodging|\n",
      "|      132655|  2016-02-18|346039430|DFINE, Inc| 114.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|      132655|  2016-04-18|346039432|DFINE, Inc|122.49|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  2016-03-15|346039434|DFINE, Inc| 103.5|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  2016-09-06|346039436|DFINE, Inc| 90.91|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  2016-02-12|346039438|DFINE, Inc| 90.87|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  2016-02-13|346039440|DFINE, Inc| 23.45|Allopathic & Oste...| Food and Beverage|\n",
      "|     1006832|  2016-02-13|346039442|DFINE, Inc|  32.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  2016-01-26|346039444|DFINE, Inc| 630.2|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  2016-01-26|346039446|DFINE, Inc|  25.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  2016-02-12|346039448|DFINE, Inc|  59.8|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  2016-02-19|346039450|DFINE, Inc| 27.27|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  2016-02-12|346039452|DFINE, Inc| 10.75|Allopathic & Oste...| Food and Beverage|\n",
      "+------------+------------+---------+----------+------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds.createOrReplaceGlobalTempView(\"payments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying and Exploring the data  \n",
    "\n",
    "## Querying can be done in two ways. One by using Spark functions and other directly by wrinting SQl statements. Both the ways are used below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 nature of accounts with payments by count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|   Nature_of_Payment|  count|\n",
      "+--------------------+-------+\n",
      "|   Food and Beverage|9975364|\n",
      "|  Travel and Lodging| 575255|\n",
      "|Compensation for ...| 251440|\n",
      "|           Education| 245515|\n",
      "|      Consulting Fee| 115316|\n",
      "|                Gift|  41523|\n",
      "|           Honoraria|  17795|\n",
      "|  Royalty or License|  11814|\n",
      "|Compensation for ...|   9171|\n",
      "|       Entertainment|   8408|\n",
      "+--------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "ds.groupBy(ds[\"Nature_of_Payment\"]).count().orderBy(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nature of payments with payments  > $1000 with their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|   Nature_of_Payment| count|\n",
      "+--------------------+------+\n",
      "|           Education|  8378|\n",
      "|       Entertainment|    30|\n",
      "|  Travel and Lodging| 24196|\n",
      "|Charitable Contri...|   235|\n",
      "|Current or prospe...|   523|\n",
      "|           Honoraria| 13052|\n",
      "|               Grant|  2154|\n",
      "|Compensation for ...|  2032|\n",
      "|  Royalty or License|  9151|\n",
      "|Compensation for ...|  5688|\n",
      "|   Food and Beverage|   430|\n",
      "|Compensation for ...|190064|\n",
      "|      Consulting Fee| 71516|\n",
      "|                Gift|  1234|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.filter(ds[\"amount\"] > 1000).groupBy(ds[\"Nature_of_Payment\"]).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top five Physicain specialites by total amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|physician_id|             revenue|\n",
      "+------------+--------------------+\n",
      "|      288926|       2.183833534E7|\n",
      "|      311622|1.9940975750000004E7|\n",
      "|      327184|1.3732734540000003E7|\n",
      "|      281659|         1.3202939E7|\n",
      "|       32719|1.2149519940000001E7|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql (\"select physician_id , sum(amount) as revenue from global_temp.payments group by physician_id order by revenue desc limit 5\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10  nature of payments by total amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|   Nature_of_payment|               total|\n",
      "+--------------------+--------------------+\n",
      "|Compensation for ...| 5.618398276899997E8|\n",
      "|  Royalty or License| 4.892801498499998E8|\n",
      "|      Consulting Fee|3.6266093983000004E8|\n",
      "|   Food and Beverage|2.4166747336000344E8|\n",
      "|  Travel and Lodging|1.9028856042000213E8|\n",
      "|Current or prospe...|6.2862144450000025E7|\n",
      "|           Honoraria|       4.392361441E7|\n",
      "|           Education| 3.895051414999967E7|\n",
      "|               Grant|       2.389616791E7|\n",
      "|Compensation for ...|       2.006624783E7|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Nature_of_payment , sum(amount) as total from global_temp.payments group by Nature_of_payment order by total desc limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Average amount of payment  in each month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number,dayofmonth,hour,dayofyear,month,year,weekofyear,date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|month(date_payment)|       avg(amount)|\n",
      "+-------------------+------------------+\n",
      "|                 12|203.47133688633699|\n",
      "|               null|               1.0|\n",
      "|                  1| 173.8491177234156|\n",
      "|                  6|162.98720391815715|\n",
      "|                  3|162.92386677567347|\n",
      "|                  5|197.61441697505896|\n",
      "|                  9|155.56513796119987|\n",
      "|                  4|199.70469070432415|\n",
      "|                  8|196.05558923083862|\n",
      "|                  7|180.39087668301877|\n",
      "|                 10|167.21379880949354|\n",
      "|                 11|213.26558497269005|\n",
      "|                  2|188.95287296417095|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.groupBy(month(ds['date_payment'])).mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-3.6.1-cp36-cp36m-win_amd64.whl (291kB)\n",
      "Installing collected packages: pymongo\n",
      "Successfully installed pymongo-3.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhic\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5acea4c034f2a33ab4b8174a\n",
      "['mytable']\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "# data base name : 'test-database-1'\n",
    "mydb = client['test-database-1']\n",
    "\n",
    "import datetime\n",
    "\n",
    "myrecord = {\n",
    "        \"author\": \"Duke\",\n",
    "        \"title\" : \"PyMongo 101\",\n",
    "        \"tags\" : [\"MongoDB\", \"PyMongo\", \"Tutorial\"],\n",
    "        \"date\" : datetime.datetime.utcnow()\n",
    "        }\n",
    "\n",
    "record_id = mydb.mytable.insert(myrecord)\n",
    "\n",
    "print (record_id)\n",
    "print (mydb.collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+----------+------+--------------------+------------------+\n",
      "|physician_id|date_payment|record_id|     payer|amount| Physician_Specialty| Nature_of_payment|\n",
      "+------------+------------+---------+----------+------+--------------------+------------------+\n",
      "|      673985|  2016-01-21|346039414|DFINE, Inc| 286.2|Allopathic & Oste...|Travel and Lodging|\n",
      "|      673985|  2016-01-21|346039416|DFINE, Inc|  25.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|      673985|  2016-02-19|346039418|DFINE, Inc| 27.27|Allopathic & Oste...|Travel and Lodging|\n",
      "|       93975|  2016-04-15|346039420|DFINE, Inc|  21.6|Allopathic & Oste...| Food and Beverage|\n",
      "|      275444|  2016-04-29|346039422|DFINE, Inc| 22.57|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  2016-02-05|346039424|DFINE, Inc| 780.7|Allopathic & Oste...|Travel and Lodging|\n",
      "|      132655|  2016-02-05|346039426|DFINE, Inc|  25.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|      132655|  2016-02-19|346039428|DFINE, Inc| 27.27|Allopathic & Oste...|Travel and Lodging|\n",
      "|      132655|  2016-02-18|346039430|DFINE, Inc| 114.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|      132655|  2016-04-18|346039432|DFINE, Inc|122.49|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  2016-03-15|346039434|DFINE, Inc| 103.5|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  2016-09-06|346039436|DFINE, Inc| 90.91|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  2016-02-12|346039438|DFINE, Inc| 90.87|Allopathic & Oste...| Food and Beverage|\n",
      "|      132655|  2016-02-13|346039440|DFINE, Inc| 23.45|Allopathic & Oste...| Food and Beverage|\n",
      "|     1006832|  2016-02-13|346039442|DFINE, Inc|  32.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  2016-01-26|346039444|DFINE, Inc| 630.2|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  2016-01-26|346039446|DFINE, Inc|  25.0|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  2016-02-12|346039448|DFINE, Inc|  59.8|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  2016-02-19|346039450|DFINE, Inc| 27.27|Allopathic & Oste...|Travel and Lodging|\n",
      "|     1006832|  2016-02-12|346039452|DFINE, Inc| 10.75|Allopathic & Oste...| Food and Beverage|\n",
      "+------------+------------+---------+----------+------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not 'RDD'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-911dc0acaa40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJSON\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\abhic\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             raise TypeError('the JSON object must be str, bytes or bytearray, '\n\u001b[0;32m--> 348\u001b[0;31m                             'not {!r}'.format(s.__class__.__name__))\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'surrogatepass'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not 'RDD'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "results = json.loads(ds.toJSON())\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "import json\n",
    "\n",
    "results =ds.toJSON()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"physician_id\":\"673985\",\"date_payment\":\"2016-01-21\",\"record_id\":\"346039414\",\"payer\":\"DFINE, Inc\",\"amount\":286.2,\"Physician_Specialty\":\"Allopathic & Osteopathic Physicians|Anesthesiology\",\"Nature_of_payment\":\"Travel and Lodging\"}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[54] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sample(0, 0.0001, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11258361"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o48.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:282)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:276)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:276)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:298)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:297)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:3195)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:3225)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-45d46cf5a404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\abhic\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1964\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s\\n%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_exception_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1965\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1966\u001b[0;31m             \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\abhic\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\abhic\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\abhic\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\abhic\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o48.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:282)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:276)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:276)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:298)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:297)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:3195)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:3225)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "ds.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
